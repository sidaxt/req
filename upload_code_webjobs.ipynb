{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import shutil\n",
    "import base64\n",
    "import time\n",
    "import subprocess\n",
    "import platform\n",
    "import openai\n",
    "import uuid\n",
    "import asyncio\n",
    "import tiktoken\n",
    "import json\n",
    "import re\n",
    "import fitz \n",
    "import io\n",
    "import hashlib\n",
    "import subprocess\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ssirish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#all from and imports\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from datetime import datetime, timedelta \n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SearchIndex, SimpleField, SearchableField\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from pptx import Presentation\n",
    "nltk.download('punkt')\n",
    "\n",
    "from collections import defaultdict\n",
    "from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader, UnstructuredPowerPointLoader,UnstructuredPDFLoader, TextLoader,Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader, PyPDFLoader\n",
    "# from langchain.document_loaders import TextLoader\n",
    "# from langchain.document_loaders import Docx2txtLoader\n",
    "# from langchain.document_loaders import UnstructuredPowerPointLoader\n",
    "\n",
    "from PIL import Image\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from quart import (\n",
    "    Blueprint,\n",
    "    Quart,\n",
    "    redirect,\n",
    "    url_for,\n",
    "    current_app,\n",
    "    jsonify,\n",
    "    make_response,\n",
    "    request,\n",
    "    send_from_directory,\n",
    "    session,\n",
    "    render_template,\n",
    "    Response\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 3\n",
      "Python-dotenv could not parse statement starting at line 7\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 18\n"
     ]
    }
   ],
   "source": [
    "# Load the .env file\n",
    "load_dotenv(\"C:/Users/ssirish/Downloads/aiprod_code_repos/AzureOpenAIProduction/app/backend/.env_local\")\n",
    "# load_dotenv(dotenv_path=\"app\\backend\\.env_local\")\n",
    "\n",
    "# Constants\n",
    "CHUNK_SIZE = 1000\n",
    "# user_name='ssirish@dsi.com' \n",
    "pattern = r\"pageno_(\\d+)\"\n",
    "CHUNK_OVERLAP = 200\n",
    "BATCH_SIZE = 100\n",
    "AZURE_STORAGE_CONNECTION_STRING = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "\n",
    "# 'ssirish@dsi.com' = 'ssirish@dsi.com'\n",
    "\n",
    "input_file = \"C:/Users/ssirish/Downloads/Sharepoint files download/Enhertu-Overview of the DAISY Trial.docx\"  # Path to your input file\n",
    "output_folder = \"C:/Users/ssirish/Downloads/Sharepoint files download/output\"  # Path to your output folder\n",
    "# output_folder = \"C:/Users/a6006/OneDrive - Axtria/Documents/DSI_New/Sharepoint files download/output\"  # Path to your output folder\n",
    "# # output_folder = \"C:\\Users\\a6006\\OneDrive - Axtria\\Documents\\DSI_New\\Sharepoint files download\\output\"  # Path to your output folder\n",
    "# # output_folder = \"output\"  # Path to your output folder\n",
    "# split_word_doc_with_images(input_file, output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ssirish@dsi.com' = 'ssirish@dsi.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\") \n",
    "    \n",
    "def img_summary(img_path):\n",
    "    try:\n",
    "        base64_image = encode_image(img_path)\n",
    "        # print(\"here******47\")\n",
    "        openai.api_type = \"azure\"\n",
    "        # openai.api_key = \"5d69dd6f0aa14121ae81066435443abf\"\n",
    "        openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        # openai.api_base = \"https://openai-poc-east.openai.azure.com/\"\n",
    "        AZURE_OPENAI_SERVICE = os.getenv(\"AZURE_OPENAI_SERVICE\")\n",
    "        openai.api_base = f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\"\n",
    "        openai.api_version = \"2024-02-15-preview\"\n",
    "        # print(\"here******52\")\n",
    "        response = openai.ChatCompletion.create(\n",
    "                    deployment_id=\"GPT4o\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"Act as an Image Analyzer\"},\n",
    "                        {\"role\": \"user\", \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": \"Generate information that is described in the image.\"},\n",
    "                            {\"type\": \"image_url\", \"image_url\": {\n",
    "                                \"url\": f\"data:image/png;base64,{base64_image}\"}\n",
    "                            }\n",
    "                        ]}\n",
    "                    ],\n",
    "                    temperature=0.0,\n",
    "                    )\n",
    "        # print(response['choices'][0]['message']['content'])\n",
    "        # print(\"here******66\")\n",
    "        if (len(response['choices'][0]['message']['content'])>0) and (response[\"usage\"][\"total_tokens\"] > 0):\n",
    "            return(response['choices'][0]['message']['content'], response[\"usage\"][\"total_tokens\"])\n",
    "        else:\n",
    "            return \"\", 0\n",
    "    except:\n",
    "        return \"\", 0\n",
    "        pass\n",
    "    #     return(\"\"\"You might have asked something out of the context with the uploaded data, If not please be more specific in the input asked\"\"\", 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "def convert_docx_to_pdf(docx_file_path, output_dir=None):\n",
    "    try:\n",
    "    # Ensure the input file exists\n",
    "        if not os.path.isfile(docx_file_path):\n",
    "            raise FileNotFoundError(f\"The file {docx_file_path} does not exist\")\n",
    " \n",
    "        # If no output directory is provided, use the same directory as the input file\n",
    "        if output_dir is None:\n",
    "            output_dir = os.path.dirname(docx_file_path)\n",
    "       \n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    " \n",
    "        # Determine the platform (Linux or Windows)\n",
    "        system_platform = platform.system()\n",
    " \n",
    "        # Build the command for LibreOffice (soffice)\n",
    "        if system_platform == \"Linux\":\n",
    "            # soffice_cmd = '/home/libreoffice/lib_office/opt/libreoffice24.2/program/soffice'  # Typically the command in Linux\n",
    "            # soffice_cmd = '/home/libreoffice/lib/program/soffice'\n",
    "            soffice_cmd = '/home/libreoffice/lib_office/opt/libreoffice24.2/program/soffice'\n",
    "            # soffice_cmd = '/usr/bin/soffice'\n",
    "            # soffice_cmd = '/opt/libreoffice24.2/program/soffice'\n",
    "        elif system_platform == \"Windows\":\n",
    "            soffice_cmd = 'C:/Program Files/LibreOffice/program/soffice.exe'\n",
    "        else:\n",
    "            raise OSError(f\"Unsupported platform: {system_platform}\")\n",
    " \n",
    "        # Output PDF path (same as input file but with .pdf extension)\n",
    "        output_pdf_path = os.path.join(output_dir, os.path.splitext(os.path.basename(docx_file_path))[0] + \".pdf\")\n",
    "        abs_docx_file_path = os.path.abspath(docx_file_path)\n",
    "        abs_output_dir = os.path.dirname(abs_docx_file_path)\n",
    "        # Construct the command to convert the file\n",
    "        command = [\n",
    "            soffice_cmd,\n",
    "            \"--headless\",  # Run in headless mode (without GUI)\n",
    "            \"--convert-to\", \"pdf\",  # Convert to PDF format\n",
    "            \"--outdir\", f\"{abs_output_dir}\",  # Specify the output directory\n",
    "            f\"{abs_docx_file_path}\"  # Input file path\n",
    "        ]\n",
    " \n",
    "       \n",
    "        # Execute the command\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"Conversion successful: {output_pdf_path}\")\n",
    "        # logging.debug(f\"Conversion successful: {output_pdf_path}\")\n",
    "        return output_pdf_path\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise RuntimeError(f\"LibreOffice failed to convert {docx_file_path} to PDF: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "def create_search_index(index_name, search_service_endpoint, search_api_key):\n",
    "    # search_service_name = \"poc-openai-cogsrch\"\n",
    "    search_service_name = os.getenv('AZURE_SEARCH_SERVICE_NAME')\n",
    "    # admin_key = AzureKeyCredential(\"TBpoXA6at3L38HsBWSRHnZWfstC07Txt7kDulY7ccDAzSeAHslm9\")\n",
    "    admin_key = AzureKeyCredential(f\"{os.getenv('AZURE_SEARCH_API_KEY')}\")\n",
    "    # index_name = \"srathore_bulk\"\n",
    "    # indexer_client = SearchIndexerClient(endpoint=f\"https://{search_service_name}.search.windows.net\", credential=admin_key)\n",
    "\n",
    "    # Initialize SearchIndexClient\n",
    "    search_service_endpoint = f\"https://{search_service_name}.search.windows.net\"\n",
    "    index_client = SearchIndexClient(endpoint=search_service_endpoint, credential=admin_key)\n",
    "    \n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=\"Edm.String\", key=True),\n",
    "        SearchableField(name=\"content\", type=\"Edm.String\", searchable=True, filterable=False, sortable=False),\n",
    "        SimpleField(name=\"filepath\", type=\"Edm.String\"),\n",
    "        SearchableField(name=\"title\", type=\"Edm.String\"),\n",
    "        SimpleField(name=\"url\", type=\"Edm.String\"),\n",
    "        SimpleField(name=\"page_number\",type=\"Edm.Int32\")\n",
    "        #SimpleField(name=\"page_number\", type=\"Edm.Int32\")\n",
    "        # VectorField(name=\"contentVector\", dimensions=1536, vector_search_configuration=\"vector_config\")\n",
    "    ]\n",
    "\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        # vector_search_configurations=[\n",
    "        #     {\"name\": \"vector_config\", \"kind\": \"vector_search\"}\n",
    "        # ]\n",
    "    )\n",
    "\n",
    "    index_client.create_index(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "def pdf_image(file_path, img_path):\n",
    "    seen_hashes = set()\n",
    "    img_present = False\n",
    "    \n",
    "    file_ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    try:\n",
    "        if file_ext == \".pdf\":\n",
    "            pdf_file = fitz.open(file_path)\n",
    "            for page_index in range(len(pdf_file)):\n",
    "                page = pdf_file[page_index]\n",
    "                image_list = page.get_images(full=True)\n",
    "\n",
    "                if image_list:\n",
    "                    img_present = True\n",
    "\n",
    "                for image_index, img in enumerate(image_list):\n",
    "                    xref = img[0]\n",
    "                    base_image = pdf_file.extract_image(xref)\n",
    "                    image_bytes = base_image[\"image\"]\n",
    "                    image_ext = base_image[\"ext\"]\n",
    "                    img_hash = hashlib.md5(image_bytes).hexdigest()\n",
    "\n",
    "                    if img_hash not in seen_hashes:\n",
    "                        seen_hashes.add(img_hash)\n",
    "                        image_name = f\"pageno_{page_index+1}_{image_index+1}.{image_ext}\"\n",
    "                        image_path = os.path.join(img_path, image_name)\n",
    "                        with open(image_path, \"wb\") as img_file:\n",
    "                            img_file.write(image_bytes)\n",
    "\n",
    "        elif file_ext == \".pptx\":\n",
    "            ppt = Presentation(file_path)\n",
    "            print(\"PPT_Image_Extraction\")\n",
    "            for slide_index, slide in enumerate(ppt.slides):\n",
    "                for shape_index, shape in enumerate(slide.shapes):\n",
    "                    try :\n",
    "                        if shape.shape_type == 13 and hasattr(shape, \"image\"):  # IMAGE shape type\n",
    "                            img_present = True\n",
    "                            img = shape.image\n",
    "                            img_data = img.blob\n",
    "                            img_hash = hashlib.md5(img_data).hexdigest()\n",
    "\n",
    "                            if img_hash not in seen_hashes:\n",
    "                                seen_hashes.add(img_hash)\n",
    "                                ext = img.ext\n",
    "                                image_name = f\"pageno_{slide_index+1}_{shape_index+1}.{ext}\"\n",
    "                                image_path = os.path.join(img_path, image_name)\n",
    "                                with open(image_path, \"wb\") as img_file:\n",
    "                                    img_file.write(img_data)\n",
    "                    except AttributeError as e:\n",
    "                     print(f\"Skipping shape on slide {slide_index+1}, shape {shape_index+1}: {str(e)}\")                \n",
    "\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {file_ext}\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "        return False\n",
    "    # remove_blank_images(img_path,threshold=5)\n",
    "    # remove_small_images(img_path, min_size=5120)\n",
    "\n",
    "    return img_present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "def connect_to_blob_storage(container_name, connection_string):\n",
    "\n",
    "    connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "    # connection_string = \"DefaultEndpointsProtocol=https;AccountName=sapocopenai;AccountKey=H30mVmhmyKpFmLUkFSKpDO3CUe+jbcG8aGZ8TgCRNTQj5Ac5HB+649BzYypyo9eW0W9BRy9Z0oFr+ASt6hAVYw==;EndpointSuffix=core.windows.net\"\n",
    "    # container_name = \"bulk-upload-qna\"\n",
    "    container_name = os.getenv('AZURE_BULK_UPLOAD_CONTAINER_NAME')\n",
    "\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    return container_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "async def load_and_split_file(path,img_there,temp_image_path):\n",
    "    if path.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(path)\n",
    "    elif path.endswith(('.doc', '.docx')):\n",
    "        loader = UnstructuredWordDocumentLoader(path)\n",
    "    elif path.endswith(('.ppt', '.pptx')):\n",
    "        loader = UnstructuredPowerPointLoader(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {path}\")\n",
    "    \n",
    "    # Load the document\n",
    "    doc = loader.load()\n",
    "    start_time_upload=time.time()\n",
    "    # Split the document into chunks with the specified chunk size and overlap\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    split_docs = text_splitter.split_documents(doc)\n",
    "\n",
    "    # Threshold for determining insufficient text content\n",
    "    token_threshold = 20\n",
    "    insufficient_text_pages = []\n",
    "    has_text = False\n",
    "    # img_docs\n",
    "\n",
    "    # Check each chunk for text content length\n",
    "    total_tokens = sum(len(chunk.page_content.split()) for chunk in split_docs)\n",
    "    has_text = total_tokens > 20\n",
    "\n",
    "    for idx, chunk in enumerate(split_docs):\n",
    "        # Handle page/slide metadata based on file type\n",
    "        # if len(chunk.page_content.split()) >= token_threshold:\n",
    "        #     has_text = True\n",
    "        if path.endswith(('.ppt', '.pptx')):\n",
    "            # print(\"Break1\")\n",
    "            chunk.metadata[\"page\"] = idx + 1\n",
    "            # print(\"Break2\")  # Using chunk index to assign slide number (starting from 1)\n",
    "        else:\n",
    "            chunk.metadata[\"page\"] = chunk.metadata.get(\"page\", 0) + 1\n",
    "\n",
    "               # print(\"Img_docs\",img_doc,\"Img_Docs\")\n",
    "        # else:\n",
    "        #     # print(\"Break3\")\n",
    "        #     # insufficient_text_pages.append(chunk.metadata[\"page\"])   \n",
    "        #     print(\"Break4\")\n",
    "    end_time_upload = time.time()\n",
    " \n",
    "    execution_time_upload = end_time_upload - start_time_upload\n",
    "    print(f\"Time taken for chunking: {execution_time_upload:.6f} seconds\")        \n",
    "                 \n",
    "    # for chunk in split_docs:\n",
    "    #     chunk.metadata[\"page\"] = chunk.metadata.get(\"page\", 0) + 1\n",
    "    #     if len(chunk.page_content.split()) >= token_threshold:\n",
    "    #         has_text = True  # Mark that the document has text\n",
    "\n",
    "    #            # print(\"Img_docs\",img_doc,\"Img_Docs\")\n",
    "    #     else:\n",
    "            # insufficient_text_pages.append(chunk.metadata[\"page\"])\n",
    "            \n",
    "\n",
    "    # scanned_doc_handler = scannned_document()\n",
    "    # scanned_doc_handler = scannned_document()\n",
    "    print(\"Break2\")\n",
    "    # Trigger OCR only if no meaningful text is found in the document\n",
    "\n",
    "    if not has_text:\n",
    "        # print(f\"Document appears to be scanned with insufficient text. Pages: {insufficient_text_pages}\")\n",
    "        start_time_upload=time.time()\n",
    "        output_folder = os.path.join(\"./temp_images\", os.path.basename(path).split('.')[0])\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Convert all pages of the document into images\n",
    "        # await scanned_doc_handler.split_pdf_into_images(path, output_folder)\n",
    "\n",
    "        combined_ocr_results = []\n",
    "        # Process the images with OCR\n",
    "        for image_file in sorted(os.listdir(output_folder)):\n",
    "            image_path = os.path.join(output_folder, image_file)\n",
    "            if os.path.isfile(image_path) and image_file.endswith(('.png', '.jpeg', '.jpg')):\n",
    "                ocr_text = '' #scanned_doc_handler.extract_text_from_image(image_path)\n",
    "                # ocr_text = scanned_doc_handler.extract_text_from_image(image_path)\n",
    "                if ocr_text.strip():  # Skip empty OCR results\n",
    "                    page_number = int(image_file.split('_')[1].split('.')[0])  # Extract page number from file name\n",
    "                    combined_ocr_results.append({\"content\": ocr_text, \"page\": page_number})\n",
    "\n",
    "        # Clean up temporary files\n",
    "        shutil.rmtree(output_folder, ignore_errors=True)\n",
    "\n",
    "        # Check if OCR results are empty\n",
    "        if not combined_ocr_results:\n",
    "            raise ValueError(\"OCR failed to extract text from the document. No data to index.\")\n",
    "        \n",
    "        end_time_upload = time.time()\n",
    " \n",
    "        execution_time_upload = end_time_upload - start_time_upload\n",
    "        print(f\"Time taken for scanned document chunking: {execution_time_upload:.6f} seconds\")\n",
    "\n",
    "        return combined_ocr_results\n",
    "    else:\n",
    "        print(f\"Document contains sufficient text. Skipping OCR.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Combine chunks by page for normal text documents\n",
    "    img_docs = []\n",
    "    if has_text:\n",
    "        if img_there:\n",
    "            start_time_upload=time.time()\n",
    "\n",
    "            files = os.listdir(temp_image_path)  \n",
    "            for j,i in enumerate(files):\n",
    "                # print(j,i)\n",
    "                # print(\"##&&&&&&&&&&&&&&&&&&\")\n",
    "                match = re.search(pattern, i)\n",
    "                if match:\n",
    "                    pageno = match.group(1)\n",
    "                img_info, img_token_info = img_summary(os.path.join(temp_image_path, i))\n",
    "                ###testing###### \n",
    "                # print(img_info)\n",
    "                img_doc = Document(img_info, metadata={'source': os.path.basename(path), 'page': pageno})\n",
    "                # print(img_doc)\n",
    "                # img_docs = text_splitter.split_documents([img_doc])\n",
    "                img_docs.extend(text_splitter.split_documents([img_doc]))\n",
    "                os.remove(os.path.join(temp_image_path, i))\n",
    "               \n",
    "                # print(\"Img_docs\",img_doc,\"Img_Docs\")\n",
    "            end_time_upload = time.time()\n",
    " \n",
    "            execution_time_upload = end_time_upload - start_time_upload\n",
    "            print(f\"Time taken for Images within document processing: {execution_time_upload:.6f} seconds\")    \n",
    "    combined_docs = []\n",
    "    split_docs_by_page = defaultdict(list)\n",
    "    img_docs_by_page = defaultdict(list)\n",
    "    if img_there:\n",
    "        for img_doc in img_docs:\n",
    "            # page = img_doc.metadata[\"page\"]\n",
    "            page = int(img_doc.metadata[\"page\"])\n",
    "            img_docs_by_page[page].append(img_doc)\n",
    "            # print(f\"@@@@@@ Image Document on Page {page}: {img_doc} !!!!!!!\")\n",
    "        # Group chunks by page number\n",
    "    \n",
    "    for doc in split_docs:\n",
    "        # page = doc.metadata[\"page\"]\n",
    "        page = int(doc.metadata[\"page\"])\n",
    "        split_docs_by_page[page].append(doc)\n",
    "        # print(f\"////// Split Document on Page {page}: {doc} ///////\")\n",
    "\n",
    "    # for page_number in sorted(map(int, split_docs_by_page.keys())):\n",
    "    #     combined_docs.extend(split_docs_by_page[page_number])\n",
    "    all_pages = sorted(set(map(int, split_docs_by_page.keys())).union(map(int, img_docs_by_page.keys())))\n",
    "    # print(f\"All Pages to Process: {all_pages}\")\n",
    "\n",
    "    for page_number in all_pages:\n",
    "        if page_number in img_docs_by_page:\n",
    "            combined_docs.extend(img_docs_by_page[page_number])\n",
    "            # print(f\"Adding Image Docs from Page {page_number}: {img_docs_by_page[page_number]}\")\n",
    "        if page_number in split_docs_by_page:\n",
    "            combined_docs.extend(split_docs_by_page[page_number])\n",
    "            # print(f\"Adding Split Docs from Page {page_number}: {split_docs_by_page[page_number]}\")\n",
    "\n",
    "    return [{\"content\": chunk.page_content, \"page\": chunk.metadata[\"page\"]} for chunk in combined_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "async def create_index(user_name):\n",
    "    container_name_bulk = os.getenv('AZURE_BULK_UPLOAD_CONTAINER_NAME')\n",
    "    container_name = f\"{container_name_bulk}\" + user_name\n",
    "    connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "    # connection_string = \"DefaultEndpointsProtocol=https;AccountName=sapocopenai;AccountKey=H30mVmhmyKpFmLUkFSKpDO3CUe+jbcG8aGZ8TgCRNTQj5Ac5HB+649BzYypyo9eW0W9BRy9Z0oFr+ASt6hAVYw==;EndpointSuffix=core.windows.net\"\n",
    "    search_service_name = os.getenv(\"AZURE_SEARCH_SERVICE_NAME\")\n",
    "    search_service_endpoint = f\"https://{search_service_name}.search.windows.net\"\n",
    "    # search_api_key =\"TBpoXA6at3L38HsBWSRHnZWfstC07Txt7kDulY7ccDAzSeAHslm9\"\n",
    "    search_api_key = os.getenv('AZURE_SEARCH_API_KEY')\n",
    "    print(\"231\", search_api_key, \"search_api_key\")\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    # index_name = f\"{user_name}_index\"\n",
    "    # index_name = f\"{user_name.split('@')[0]}-index\"\n",
    "    # index_name = \"mparag-index\"\n",
    "    # index_name = f\"{user_name.split('@')[0]}-index\"\n",
    "    index_name1 = re.sub(r'[^a-zA-Z0-9]', '', user_name.split('@')[0])\n",
    "    index_name = f\"{index_name1}\"\n",
    "    print(index_name,\"Index_Name\")\n",
    "\n",
    "    index_client = SearchIndexClient(endpoint=search_service_endpoint, credential=AzureKeyCredential(search_api_key))\n",
    "\n",
    "    # Check if the index already exists\n",
    "    index_exists = False\n",
    "    try:\n",
    "        index_exists = index_client.get_index(index_name) is not None\n",
    "    except Exception:\n",
    "        # Index does not exist, will create a new one\n",
    "        pass\n",
    "\n",
    "    # Create the index if it does not exist\n",
    "    if not index_exists:\n",
    "        create_search_index(index_name, search_service_endpoint, search_api_key)\n",
    "                            \n",
    "    search_client = SearchClient(\n",
    "        endpoint=search_service_endpoint,\n",
    "        index_name=index_name,\n",
    "        credential=AzureKeyCredential(search_api_key)\n",
    "    )\n",
    "    container_client = connect_to_blob_storage(container_name, connection_string)\n",
    "    # print(\"Hello1\")\n",
    "    blobs = container_client.list_blobs()\n",
    "    # # print(\"Hello2\")\n",
    "    # STORAGE_ACCOUNT_NAME=\"sapocopenai\"\n",
    "    # STORAGE_ACCOUNT_KEY=\"H30mVmhmyKpFmLUkFSKpDO3CUe+jbcG8aGZ8TgCRNTQj5Ac5HB+649BzYypyo9eW0W9BRy9Z0oFr+ASt6hAVYw==\"\n",
    "\n",
    "    # temp_folder_path = os.path.join(\"..\", \"./temp\")\n",
    "\n",
    "    sharepoint_tmp_folder_path = os.path.join(\"sharepoint_tmp_folder\", 'ssirish@dsi.com')\n",
    "    tempfile_path = os.path.join(\"tempfile\", 'ssirish@dsi.com')\n",
    "\n",
    "    # Dynamically select the folder based on existence\n",
    "    selected_folder_path = None\n",
    "\n",
    "    if os.path.exists(sharepoint_tmp_folder_path):\n",
    "        selected_folder_path = sharepoint_tmp_folder_path\n",
    "        print(f\"Using 'sharepoint_tmp_folder': {selected_folder_path}\")\n",
    "    elif os.path.exists(tempfile_path):\n",
    "        selected_folder_path = tempfile_path\n",
    "        print(f\"Using 'tempfile': {selected_folder_path}\")\n",
    "        temp_folder_path = os.path.join(\"sharepoint_tmp_folder\", 'ssirish@dsi.com')\n",
    "    else:\n",
    "        print(\"No temporary folders found.\")\n",
    "    # Handle the case where neither folder exists\n",
    "        selected_folder_path = None  # or raise an exception, return a specific response, etc.\n",
    "\n",
    "# Proceed with file processing if a folder was found\n",
    "    if selected_folder_path:\n",
    "        # Add your file processing logic here\n",
    "        print(f\"Processing files in: {selected_folder_path}\")\n",
    "\n",
    "    # temp_folder_path = \"sharepoint_tmp_folder/adeorukhkar@dsi.com\"\n",
    "    # temp_image_path1=\"./temp_image\"\n",
    "    temp_image_path = os.path.join(\"./temp_image\", 'ssirish@dsi.com')\n",
    "    os.makedirs(temp_image_path, exist_ok=True)\n",
    "    for file_name in os.listdir(selected_folder_path):\n",
    "        print(\"inside for loop indexcreation\")\n",
    "        file_path = os.path.join(selected_folder_path, file_name)\n",
    "        print(file_name,\"Filename\")\n",
    "        \n",
    "        # Only process valid files\n",
    "        if os.path.isfile(file_path):\n",
    "            container_name = os.getenv('AZURE_BULK_UPLOAD_CONTAINER_NAME')\n",
    "            # container_name = \"bulk-upload-qna\"\n",
    "            user_name = user_name\n",
    "            title = os.path.splitext(file_name)[0]\n",
    "            url = f\"https://sapocopenai.blob.core.windows.net/{container_name}/{user_name}/{title}.pdf\"\n",
    "            #url2 = await generate_previewable_url(container_name,title)\n",
    "            batch = []\n",
    "\n",
    "            # Load and split the file into chunks\n",
    "            # temp_image_path=\"./temp_image\"\n",
    "            temp_image_path = os.path.join(\"./temp_image\", 'ssirish@dsi.com')\n",
    "            os.makedirs(temp_image_path, exist_ok=True)\n",
    "            img_there=pdf_image(file_path,temp_image_path)\n",
    "            chunks = await load_and_split_file(file_path,img_there,temp_image_path)\n",
    "            if os.path.exists(os.path.join(\"temp_image\", 'ssirish@dsi.com')):\n",
    "            # print(\"yes database folder\")\n",
    "                await asyncio.get_event_loop().run_in_executor(None, shutil.rmtree, os.path.join(\"temp_image\", 'ssirish@dsi.com')) \n",
    "            \n",
    "            for chunk in chunks:\n",
    "                        page_no = chunk[\"page\"]\n",
    "                        document = {\n",
    "                            \"id\": str(uuid.uuid4()),\n",
    "                            \"content\":chunk[\"content\"],\n",
    "                            \"title\": title,\n",
    "                            \"url\": url,\n",
    "                            # \"page_number\": chunk[\"metadata\"].get(\"page_number\", 1)\n",
    "                            #\"page_number\": chunk[\"page_no\"],\n",
    "                            \"page_number\": {page_no},\n",
    "                            \"page_number\": chunk.get(\"page\"),\n",
    "                            \"url\": f\"{url}#page={page_no}\"\n",
    "                            #\"page_number\": chunk.metadata.get(\"page_number\", 1)  # Include page number\n",
    "                            #\"page_number\": chunk.get(\"metadata\", {}).get(\"page_number\", 1)\n",
    "                        }\n",
    "                        batch.append(document)\n",
    "                        # if len(batch) >= BATCH_SIZE:\n",
    "                        #    search_client.upload_documents(batch)\n",
    "                            # batch = []\n",
    "\n",
    "        # Upload any remaining documents in the last batch\n",
    "        search_client.upload_documents(batch)\n",
    "            # if batch:\n",
    "                # upload_chunks_in_batches(search_client, batch)\n",
    "\n",
    "        # print(f\"Processed and indexed file: {blob_name}\")\n",
    "\n",
    "    print(f\"Index creation and data upload completed for user {user_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "async def upload_file_to_azure(container_name, file_path, connection_string, directory_path):\n",
    "    # Create a BlobServiceClient using the connection string\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    # logging.debug(blob_service_client, \"blob_service_client\")\n",
    "    # Create a ContainerClient\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    # logging.debug(container_client, \"container_client\")\n",
    "   \n",
    "    # Create the container if it does not exist\n",
    "    try:\n",
    "        container_client.create_container()\n",
    "    except Exception as e:\n",
    "        print(f\"Container {container_name} already exists.\")\n",
    " \n",
    "    # Get the filename from the file path\n",
    "    blob_name = os.path.basename(file_path)\n",
    "\n",
    "    if directory_path:\n",
    "        blob_name = os.path.join(directory_path, blob_name).replace(\"\\\\\", \"/\")\n",
    "\n",
    "    # Create a BlobClient\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "   \n",
    "    # Upload the file\n",
    "    with open(file_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(data, overwrite=True)\n",
    "        print(f\"File {file_path} uploaded to container {container_name} as blob {blob_name}\")\n",
    "        print(f\"{blob_client} blob_client after upload blob\")\n",
    "        # logging.debug(f\"File {file_path} uploaded to container {container_name} as blob {blob_name}\")\n",
    "        # logging.debug(f\"{blob_client} blob_client after upload blob\")\n",
    " \n",
    "async def generate_previewable_url(container_name, blob_name, expiry_hours=10):\n",
    "    \n",
    "    try:\n",
    "        blob_service_client_prev = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)\n",
    "        # Create a BlobServiceClient\n",
    "        # blob_service_client = BlobServiceClient(account_url=f\"https://{blob_service_client.account_name}.blob.core.windows.net\", credential=blob_service_client.account_key)\n",
    "        print(blob_service_client_prev, \"blob_service_client_prev\")\n",
    "        # logging.debug((blob_service_client_prev, \"blob_service_client_prev\"))\n",
    "        # Set the expiration time for the SAS token\n",
    "        sas_expiry_time = datetime.now().astimezone() + timedelta(hours=expiry_hours)\n",
    "        print(sas_expiry_time, \"sas_expiry_time\")\n",
    "        # logging.debug(sas_expiry_time, \"sas_expiry_time\")\n",
    "        \n",
    "        # Generate a SAS token for the blob\n",
    "        sas_token = generate_blob_sas(\n",
    "            account_name=blob_service_client_prev.account_name,\n",
    "            container_name=container_name,\n",
    "            blob_name=blob_name,\n",
    "            account_key=blob_service_client_prev.credential.account_key,\n",
    "            permission=BlobSasPermissions(read=True),  # Set read permissions\n",
    "            expiry=sas_expiry_time,\n",
    "            content_disposition=\"inline\",# Force the file to open in the browser\n",
    "            content_type=\"application/pdf\"\n",
    "        )\n",
    "        # logging.debug(sas_token, \"sas_token\")\n",
    "        print(sas_token, \"sas_token\")\n",
    "        # content_settings = ContentSettings(content_type=\"application/pdf\", content_disposition=\"inline\") \n",
    "        # Update blob headers if not already set correctly (optional, for existing files)\n",
    "        # blob_client_prev = blob_service_client_prev.get_blob_client(container=container_name, blob=blob_name)\n",
    "        # logging.debug(blob_client_prev, \"blob_client_prev 183\")\n",
    "        # blob_client_prev.set_http_headers(content_settings=content_settings)\n",
    "        # logging.debug(blob_client_prev, \"blob_client_prev 185\")\n",
    "        # Generate the full URL with the SAS token\n",
    "        print(container_name, \"container_name before blob_url\", blob_name, \"blob_name before blob_url\", sas_token, \"sas_token before blob_url\")\n",
    "        # logging.debug(container_name, \"container_name before blob_url\", blob_name, \"blob_name before blob_url\", sas_token, \"sas_token before blob_url\")\n",
    "        blob_url = f\"https://{blob_service_client_prev.account_name}.blob.core.windows.net/{container_name}/{blob_name}?{sas_token}\"\n",
    "        # blob_url = f\"https://{blob_service_client_prev.account_name}.blob.core.windows.net/{container_name}/{blob_name}\"\n",
    "        print(blob_url)\n",
    "        # logging.debug(blob_url, \"blob_url 200\" )\n",
    "        return blob_url\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate the URL: {e}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "def token_counter(file):\n",
    "    AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY_East\")\n",
    "    AZURE_OPENAI_SERVICE = os.getenv(\"AZURE_OPENAI_SERVICE_East\")\n",
    "    openai_api_type = \"azure\"\n",
    "    openai_api_version = \"2023-12-01-preview\" \n",
    "    openai_api_base = f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\" \n",
    "    openai_api_key = AZURE_OPENAI_API_KEY\n",
    "    embeddings = OpenAIEmbeddings(openai_api_base = openai_api_base,\n",
    "                              openai_api_type = \"azure\",\n",
    "                              openai_api_key=openai_api_key, \n",
    "                              openai_api_version = openai_api_version, \n",
    "                              deployment=\"text-embedding-ada-002\", \n",
    "                              chunk_size = 1)\n",
    "    if file.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file)\n",
    "        doc = loader.load()\n",
    "    if file.endswith('.txt'):\n",
    "        loader = TextLoader(file)\n",
    "        doc = loader.load()\n",
    "    if file.endswith('.docx'):\n",
    "        loader = Docx2txtLoader(file)\n",
    "        doc = loader.load()\n",
    "    if file.endswith('.pptx'):\n",
    "        loader = UnstructuredPowerPointLoader(file)\n",
    "        doc = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap  = 200)\n",
    "    docs = text_splitter.split_documents(doc)\n",
    "    input_str = \"\"\n",
    "    enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "    for i in range(len(docs)):\n",
    "        input_str = input_str + docs[i].page_content\n",
    "    \n",
    "    return len(enc.encode(input_str)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import queue\n",
    "# from spire.doc import *\n",
    "# from spire.doc.common import *\n",
    "\n",
    "# def docx_image(file_path, img_path):\n",
    "#     try:\n",
    "#         document = Document()\n",
    "#         document.LoadFromFile(file_path)\n",
    "#         images = []\n",
    "\n",
    "#         nodes = queue.Queue()\n",
    "#         nodes.put(document)\n",
    "\n",
    "#         while not nodes.empty():\n",
    "#             node = nodes.get()\n",
    "#             for i in range(node.ChildObjects.Count):\n",
    "#                 obj = node.ChildObjects[i]\n",
    "#                 # Find the images\n",
    "#                 if isinstance(obj, DocPicture):\n",
    "#                     picture = obj\n",
    "#                     # Append the image data to the list\n",
    "#                     data_bytes = picture.ImageBytes\n",
    "#                     images.append(data_bytes)\n",
    "#                 elif isinstance(obj, ICompositeObject):\n",
    "#                     nodes.put(obj)\n",
    "#         for i, image_data in enumerate(images):\n",
    "#             file_name = f\"Image-{i}.png\"\n",
    "#             with open(os.path.join(img_path, file_name), 'wb') as image_file:\n",
    "#                 image_file.write(image_data)\n",
    "\n",
    "#         document.Close()\n",
    "#         if len(images) > 0:\n",
    "#             image_flag = True\n",
    "#         return image_flag\n",
    "#     except:\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "async def create_vectorstore(file, user_name, file_url):\n",
    "    AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY_East\")\n",
    "    AZURE_OPENAI_SERVICE = os.getenv(\"AZURE_OPENAI_SERVICE_East\")\n",
    "    openai_api_type = \"azure\"\n",
    "   # openai_api_version = \"2023-03-15-preview\"\n",
    "    openai_api_version = \"2023-12-01-preview\"  \n",
    "    openai_api_base = f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\"\n",
    "    openai_api_key = AZURE_OPENAI_API_KEY\n",
    "    embeddings = OpenAIEmbeddings(openai_api_base = openai_api_base,\n",
    "                              openai_api_type = \"azure\",\n",
    "                              openai_api_key=openai_api_key,\n",
    "                              openai_api_version = openai_api_version,\n",
    "                              deployment=\"text-embedding-ada-002\",\n",
    "                              #chunk_size = 1,\n",
    "                            #   max_retries = 0)\n",
    "                                )\n",
    "    doc_flag = False\n",
    " \n",
    "    if file.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file)\n",
    "        doc = loader.load()\n",
    " \n",
    "    if file.endswith('.txt'):\n",
    "        pdf_path = convert_docx_to_pdf(file, None)\n",
    "        # logging.debug(\"pdf_path\", pdf_path)\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        # print(loader, \"loader\")\n",
    "        doc = loader.load()\n",
    "        # print(doc, \"doc\")\n",
    " \n",
    "    if file.endswith('.docx'):\n",
    "        doc_flag = True\n",
    "        \n",
    "        pdf_path = convert_docx_to_pdf(file, None)\n",
    "        \n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        # print(loader, \"loader\")\n",
    "        doc = loader.load()\n",
    "        # print(doc, \"doc\")\n",
    "        pdf_file_path = os.path.splitext(file)[0] + \".pdf\"\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "        file = pdf_file_path\n",
    " \n",
    "    if file.endswith('.pptx'):\n",
    "        pdf_path = convert_docx_to_pdf(file, None)\n",
    "        # logging.debug(\"pdf_path\", pdf_path)\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        # print(loader, \"loader\")\n",
    "        doc = loader.load()\n",
    "        # print(doc, \"doc\")\n",
    "       \n",
    "    if file.endswith(\".pdf\"):\n",
    "        outer_dict = {}\n",
    "        for j, i in enumerate(doc):\n",
    "            page_no = i.metadata[\"page\"] + 1\n",
    "            inner_dict = {\"content\" : i.page_content, \"source_url\": file_url, \"page_url\" : f\"{file_url}#page={page_no}\"}\n",
    "            outer_dict[page_no] = inner_dict\n",
    "            print(\"outer_dict\",outer_dict)\n",
    "       \n",
    "        os.makedirs(os.path.join(\"temp_dir_2\", user_name), exist_ok=True)\n",
    "        with open(f'./temp_dir_2/{user_name}/{os.path.basename(os.path.splitext(file)[0] + \".json\")}', 'w') as json_file:\n",
    "            json.dump(outer_dict, json_file)\n",
    " \n",
    "   # print(\"Doc Created\")\n",
    "    global count\n",
    "    count += 1\n",
    "    index_flag = False\n",
    " \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap  = 200)\n",
    "    docs = text_splitter.split_documents(doc)\n",
    "    \n",
    "    for j, i in enumerate(docs):\n",
    "        try:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                page_number = i.metadata[\"page\"]\n",
    "                # print(page_number, \"page_number\")\n",
    "                page_number = page_number + 1\n",
    "                pdf_url_with_page = f\"{file_url}#page={page_number}\"\n",
    "                # print(pdf_url_with_page, \"pdf_url_with_page\")\n",
    "                i.metadata[\"page\"] = pdf_url_with_page\n",
    "                i.metadata[\"source\"] = file_url\n",
    " \n",
    "            if file.endswith(\".txt\"):\n",
    "                i.metadata[\"source\"] = os.path.basename(file)\n",
    "        except:\n",
    "            pass\n",
    " \n",
    "    vector_store_doc = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
    " \n",
    "    # print(\"Docs created\", docs)\n",
    "   \n",
    "    timestamp = datetime.now()\n",
    "    # enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "    input_str = \"\"\n",
    "    for i in range(len(docs)):\n",
    "        input_str = input_str + docs[i].page_content\n",
    " \n",
    "    Docs = []\n",
    "    ## Checking if image present and extracting information\n",
    "    img_flag = False\n",
    "    tempdir = \"./tempfile\"\n",
    "    img_desc = \"\"\n",
    "    img_tokens = 0\n",
    "    pattern = r\"pageno_(\\d+)\"\n",
    "    pageno = \"\"\n",
    "    vector_dict = {}\n",
    " \n",
    "    os.makedirs(os.path.join(tempdir, 'ssirish@dsi.com',\"images\"), exist_ok=True)\n",
    "    img_path = os.path.join(tempdir, 'ssirish@dsi.com',\"images\")\n",
    "   \n",
    "    if file.endswith('.pdf'):\n",
    "        img_flag = pdf_image(file, img_path)\n",
    "   \n",
    "    # elif file.endswith('.pptx'):\n",
    "    #     prs = Presentation(file)\n",
    "    #     extractor = PPTExtractor(img_path)\n",
    "    #     img_flag = extractor.iter_picture_shapes(prs)\n",
    " \n",
    "    # elif file.endswith('.docx'):\n",
    "    #     img_flag = docx_image(file, img_path)\n",
    " \n",
    "    print(img_flag)\n",
    "    start_time = time.time()\n",
    "    if img_flag:\n",
    "        files = os.listdir(img_path)  \n",
    "        for j,i in enumerate(files):\n",
    "            # print(j,i)\n",
    "            match = re.search(pattern, i)\n",
    "            if match:\n",
    "                pageno = match.group(1)\n",
    "            img_info, img_token_info = img_summary(os.path.join(img_path, i))\n",
    "            ###testing######\n",
    "            # print(img_info)\n",
    "            img_doc = Document(img_info, metadata={'source': os.path.basename(file), 'page': pageno})\n",
    "            # print(img_doc)\n",
    "            img_docs = text_splitter.split_documents([img_doc])\n",
    "            vector_dict[f\"vector_store_{j}\"] = FAISS.from_documents(documents=img_docs, embedding=embeddings)\n",
    "            # print(img_docs)\n",
    "            if j == 0:\n",
    "                vector_store = vector_dict[f\"vector_store_{j}\"]\n",
    "            else:\n",
    "                vector_store.merge_from(vector_dict[f\"vector_store_{j}\"])\n",
    " \n",
    "            img_desc = img_desc + img_info\n",
    "            img_tokens = img_tokens + img_token_info\n",
    "            \n",
    "    shutil.rmtree(img_path)\n",
    "    end_time = time.time()\n",
    "    print(\"************************************************************************************************\")\n",
    "    # print(end_time - start_time)\n",
    " \n",
    "    input_str += img_desc\n",
    "    print(\"*******************************************************\")\n",
    "    # print(input_str)\n",
    "    enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "    embedding_tokens = len(enc.encode(input_str))\n",
    "    \n",
    "    print(\"image_text_extracted\")\n",
    "    \n",
    "    print(\"img_document_created\")\n",
    "    \n",
    "    path = os.path.join(\"vector_stored\", user_name, \"faiss_index_{}\".format(count))\n",
    " \n",
    "    if img_flag:\n",
    "        vector_store.merge_from(vector_store_doc)\n",
    "        vector_store.save_local(path)\n",
    "    else:\n",
    "        vector_store_doc.save_local(path)\n",
    " \n",
    "    if doc_flag:\n",
    "        os.remove(file)\n",
    " \n",
    "def extract_slide_content_with_numbers(pptx_path):\n",
    "    presentation = Presentation(pptx_path)\n",
    "    slide_data = []\n",
    " \n",
    "    for i, slide in enumerate(presentation.slides):\n",
    "        slide_number = i + 1\n",
    "        slide_content = \"\"\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                slide_content += shape.text + \"\\n\"\n",
    "            elif shape.shape_type == 19:  # Shape type 19 corresponds to a table\n",
    "                table = shape.table\n",
    "                for row in table.rows:\n",
    "                    for cell in row.cells:\n",
    "                        slide_content += cell.text + \"\\t\"\n",
    "                    slide_content += \"\\n\"\n",
    "        slide_data.append({\n",
    "            \"slide_number\": slide_number,\n",
    "            \"content\": slide_content.strip()\n",
    "        })\n",
    " \n",
    "    return slide_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "error occured: string indices must be integers, not 'str'\n"
     ]
    }
   ],
   "source": [
    "# done - upload()\n",
    "# async def upload():\n",
    "try:\n",
    "    # global img_count\n",
    "    start_time_upload_total=time.time()\n",
    "\n",
    "    files = \"C:/Users/ssirish/Downloads\"\n",
    "    # files = await request.files\n",
    "    # print(files, \"upload files\")\n",
    "    tempdir = \"./tempfile\"\n",
    "    # os.makedirs('ssirish@dsi.com', exist_ok=True)\n",
    "    os.makedirs(os.path.join(tempdir, 'ssirish@dsi.com'), exist_ok=True)\n",
    "\n",
    "    unstructured_extensions = (\".pdf\", \".docx\", \".pptx\", \".txt\")\n",
    "    stored_img_flag = False\n",
    "    total_size = 0\n",
    "    # unstructured_files = []\n",
    "    \n",
    "    #tempath = os.path.join(tempdir, 'ssirish@dsi.com', file.filename)\n",
    "    folder_files = os.listdir(os.path.join(tempdir, 'ssirish@dsi.com'))\n",
    "    print(folder_files)\n",
    "    # folder_files = os.listdir(os.path.join(tempdir, output_folder))\n",
    "    file_name = None\n",
    "    \n",
    "\n",
    "    file_count = 0\n",
    "    csv_excel_file_count = 0\n",
    "    csv_excel_flag = False\n",
    "    img_flag = False\n",
    "    unstructured_flag = False\n",
    "\n",
    "    for file in files:\n",
    "        file_count += 1\n",
    "        print(f\"Processing file: {files[file].filename}\")\n",
    "        \n",
    "        if files[file].filename.endswith(unstructured_extensions):\n",
    "            unstructured_flag = True\n",
    "\n",
    "    print(\"File_count\",file_count)\n",
    "    \n",
    "        \n",
    "    for file in files:\n",
    "        print(f\"Processing file2: {files[file].filename}\")\n",
    "        img_flag = False\n",
    "        csv_excel_flag = False\n",
    "        if img_flag:\n",
    "            _, file_extension = os.path.splitext(files[file].filename)\n",
    "        \n",
    "        user_dir = os.path.join(tempdir, 'ssirish@dsi.com')\n",
    "        # user_dir = os.path.join(tempdir, output_folder)\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(user_dir, exist_ok=True)\n",
    "\n",
    "        # Save the file\n",
    "        file_path = os.path.join(user_dir, files[file].filename)\n",
    "        await files[file].save(file_path)\n",
    "\n",
    "        # Calculate the total size of all files in the directory\n",
    "        total_size = 0\n",
    "\n",
    "        # Iterate over all files in the user directory\n",
    "    for file_name in os.listdir(user_dir):\n",
    "        file_path = os.path.join(user_dir, file_name)\n",
    "        \n",
    "        # Check if it's a file (not a directory)\n",
    "        if os.path.isfile(file_path):\n",
    "            # Add the file size to the total\n",
    "            total_size += os.path.getsize(file_path)\n",
    "            print(total_size)\n",
    "\n",
    "    if files[file].filename.endswith(unstructured_extensions):\n",
    "        file_path_list = []\n",
    "        files = os.listdir(os.path.join(tempdir, 'ssirish@dsi.com'))\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(os.path.join(tempdir, 'ssirish@dsi.com'), file_name)               \n",
    "            file_path_list.append(file_path) \n",
    "        \n",
    "        for i in file_path_list:\n",
    "            if i.endswith('.docx')  or i.endswith('.txt'):\n",
    "                pdf_path = convert_docx_to_pdf(i, None)\n",
    "                if os.path.exists(i):\n",
    "                    os.remove(i)\n",
    "                i = pdf_path  # Update the file path to the PDF path\n",
    "                print(i, \"i 688\")# if i.endswith('.docx') or i.endswith('.pptx') or i.endswith('.txt'):\n",
    "            \n",
    "\n",
    "        # Call create_index function for this file\n",
    "        await create_index(user_name='ssirish@dsi.com')\n",
    "        for i in file_path_list:\n",
    "            if i.endswith('.pptx') :\n",
    "                pdf_path = convert_docx_to_pdf(i, None)\n",
    "                if os.path.exists(i):\n",
    "                    os.remove(i)\n",
    "                i = pdf_path\n",
    "                print(i, \"i 688\")# if i.endswith('.docx') or i.endswith('.pptx') or i.endswith('.txt'):\n",
    "            \n",
    "\n",
    "            # source_container_name = \"bulk-upload-qna\"\n",
    "            source_container_name = os.getenv('AZURE_BULK_UPLOAD_CONTAINER_NAME')\n",
    "            connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "            # connection_string = \"DefaultEndpointsProtocol=https;AccountName=sapocopenai;AccountKey=H30mVmhmyKpFmLUkFSKpDO3CUe+jbcG8aGZ8TgCRNTQj5Ac5HB+649BzYypyo9eW0W9BRy9Z0oFr+ASt6hAVYw==;EndpointSuffix=core.windows.net\"\n",
    "            await upload_file_to_azure(source_container_name, i, connection_string, 'ssirish@dsi.com')\n",
    "            print(\"here 594\")\n",
    "            blob_name = 'ssirish@dsi.com' + \"/\" + os.path.basename(i)\n",
    "        \n",
    "            blob_folder = \"blob/\" + 'ssirish@dsi.com'\n",
    "\n",
    "        source_container_name = os.getenv('AZURE_BULK_UPLOAD_CONTAINER_NAME')\n",
    "        connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "        # source_container_name = \"bulk-upload-qna\"\n",
    "        # connection_string = \"DefaultEndpointsProtocol=https;AccountName=sapocopenai;AccountKey=H30mVmhmyKpFmLUkFSKpDO3CUe+jbcG8aGZ8TgCRNTQj5Ac5HB+649BzYypyo9eW0W9BRy9Z0oFr+ASt6hAVYw==;EndpointSuffix=core.windows.net\"\n",
    "        # os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "        for file_name in os.listdir(user_dir):\n",
    "            file_path = os.path.join(user_dir, file_name)\n",
    "\n",
    "            # Check if it's a file (not a directory)\n",
    "            if os.path.isfile(file_path):\n",
    "            # Upload the current file to Azure Blob Storage\n",
    "                await upload_file_to_azure(source_container_name, file_path, connection_string, 'ssirish@dsi.com')\n",
    "        end_time_upload_total = time.time()\n",
    "\n",
    "        execution_time_upload = end_time_upload_total - start_time_upload_total\n",
    "        print(f\"Time taken for document Upload: {execution_time_upload:.6f} seconds\")\n",
    "\n",
    "    tempath = os.path.join(tempdir, 'ssirish@dsi.com', files[file].filename)\n",
    "        \n",
    "    print(csv_excel_flag, \"excel_csv_flag\")\n",
    "    if (not csv_excel_flag) and (not img_flag):\n",
    "        file_path_list = []\n",
    "        scanned_file_path_list = []\n",
    "        token_limit_crossed = False\n",
    "        max_token_limit = 1000000\n",
    "        total_tokens = 0\n",
    "        files = os.listdir(os.path.join(tempdir, 'ssirish@dsi.com'))  \n",
    "        print(files, \"652\")        \n",
    "\n",
    "        for file_name in files:\n",
    "            file_tokens = 0\n",
    "            print(file_name, \"filename inside upload\")\n",
    "            file_path = os.path.join(os.path.join(tempdir, 'ssirish@dsi.com'), file_name)\n",
    "            # file_path = os.path.join(file_name)\n",
    "            print(file_path, \"file_path 659\")\n",
    "\n",
    "            file_tokens = token_counter(file_path)\n",
    "            total_tokens = total_tokens + file_tokens\n",
    "            \n",
    "            if file_tokens < 10:\n",
    "                scanned_file_path_list.append(file_path)\n",
    "                print(scanned_file_path_list, \"scanned_file_path_list\")\n",
    "            else:\n",
    "                file_path_list.append(file_path)\n",
    "                print(file_path_list, \"file_path_list\")\n",
    "\n",
    "            if total_tokens > max_token_limit:\n",
    "                token_limit_crossed = True\n",
    "                shutil.rmtree(os.path.join(tempdir, 'ssirish@dsi.com'))\n",
    "        \n",
    "        if not token_limit_crossed:\n",
    "            \n",
    "            for i in file_path_list:\n",
    "                if i.endswith('.docx') or i.endswith('.pptx') or i.endswith('.txt'):\n",
    "                    pdf_path = convert_docx_to_pdf(i, None)\n",
    "                    if os.path.exists(i):\n",
    "                        # print(\"Hello122\")\n",
    "                        os.remove(i)\n",
    "                    i = pdf_path\n",
    "                    # print(i, \"i 688\")\n",
    "\n",
    "                source_container_name = os.getenv('AZURE_TARGET_CONTAINER_NAME_FILE_UPLOAD')\n",
    "                connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "                await upload_file_to_azure(source_container_name, i, connection_string, 'ssirish@dsi.com')\n",
    "                print(\"here 594\")\n",
    "                blob_name = 'ssirish@dsi.com' + \"/\" + os.path.basename(i)\n",
    "                file_url = await generate_previewable_url(source_container_name, blob_name)\n",
    "                # file_url = await generate_target_uri(source_container_name,files[file].filename, \"\", 'ssirish@dsi.com')\n",
    "                print(\"*****************file_url*************************\")\n",
    "               \n",
    "            \n",
    "                vector_store = await create_vectorstore(file = i, user_name = 'ssirish@dsi.com', file_url = file_url)                                        \n",
    "                    \n",
    "                if os.path.exists(i):\n",
    "                    os.remove(i)\n",
    "                    # print(\"file_path deleted\", i)\n",
    "            for j, i in enumerate(scanned_file_path_list):\n",
    "                folder_path = os.path.join(tempdir, 'ssirish@dsi.com', os.path.splitext(os.path.basename(i))[0])\n",
    "                if not os.path.exists(folder_path):\n",
    "                    os.makedirs(folder_path)\n",
    "                # await scannned_document().create_vstore(i, folder_path, 'ssirish@dsi.com')\n",
    "                if os.path.exists(i):\n",
    "                    os.remove(i)\n",
    "\n",
    "    end_time_upload_total = time.time()\n",
    "\n",
    "    execution_time_upload = end_time_upload_total - start_time_upload_total\n",
    "    print(f\"Time taken for document Upload: {execution_time_upload:.6f} seconds\")\n",
    "    # return jsonify({\"status\": \"success\", \"test_purpose\": \"test_purpose\"}), 200\n",
    "except Exception as e:\n",
    "    tempdir = \"./tempfile\"\n",
    "    # remove_directory(os.path.join(tempdir, 'ssirish@dsi.com'))\n",
    "    print(\"error occured:\", e)\n",
    "    # return jsonify(status='try_after_some_time', error=str(e)), 600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object upload at 0x00000211BF9F4440>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
